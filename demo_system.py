#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†å›¾è°±ç³»ç»Ÿæ¼”ç¤ºè„šæœ¬
æ¼”ç¤ºå®ä½“å…³ç³»æŠ½å–ã€æœºå™¨å­¦ä¹ å¢å¼ºã€é‚®ä»¶çŸ¥è¯†æŠ½å–ç­‰åŠŸèƒ½
"""

import sys
import os
import json
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def test_document_parser():
    """æµ‹è¯•æ–‡æ¡£è§£æåŠŸèƒ½"""
    print("\n=== æµ‹è¯•æ–‡æ¡£è§£æåŠŸèƒ½ ===")
    
    try:
        from src.knowledge_management.infrastructure.document_parser import DocumentParserFactory
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_content = """è‹¹æœå…¬å¸æ˜¯ä¸€å®¶ç¾å›½è·¨å›½ç§‘æŠ€å…¬å¸ï¼Œæ€»éƒ¨ä½äºåŠ åˆ©ç¦å°¼äºšå·åº“æ¯”è’‚è¯ºã€‚
è’‚å§†Â·åº“å…‹æ˜¯è‹¹æœå…¬å¸çš„é¦–å¸­æ‰§è¡Œå®˜ã€‚
è¯¥å…¬å¸ä»¥è®¾è®¡å’Œåˆ¶é€ æ¶ˆè´¹ç”µå­äº§å“è€Œé—»åï¼ŒåŒ…æ‹¬iPhoneã€iPadã€Macç”µè„‘ç­‰ã€‚
è‹¹æœå…¬å¸æˆç«‹äº1976å¹´ï¼Œç”±å²è’‚å¤«Â·ä¹”å¸ƒæ–¯ã€å²è’‚å¤«Â·æ²ƒå…¹å°¼äºšå…‹å’Œç½—çº³å¾·Â·éŸ¦æ©å…±åŒåˆ›ç«‹ã€‚"""
        
        test_file = "test_document.txt"
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)
        
        # è§£ææ–‡æ¡£
        parser_factory = DocumentParserFactory()
        result = parser_factory.parse_document(test_file)
        
        print(f"âœ“ æ–‡æ¡£è§£ææˆåŠŸ")
        print(f"  - æ–‡ä»¶ç±»å‹: {result['metadata']['file_type']}")
        print(f"  - æ–‡ä»¶å¤§å°: {result['metadata']['file_size']} å­—èŠ‚")
        print(f"  - å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
        print(f"  - å†…å®¹é¢„è§ˆ: {result['content'][:100]}...")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        os.remove(test_file)
        
        return True
        
    except Exception as e:
        print(f"âœ— æ–‡æ¡£è§£ææµ‹è¯•å¤±è´¥: {e}")
        return False

def test_nlp_processor():
    """æµ‹è¯•NLPå¤„ç†åŠŸèƒ½"""
    print("\n=== æµ‹è¯•NLPå¤„ç†åŠŸèƒ½ ===")
    
    try:
        from src.knowledge_management.infrastructure.nlp_processor import ChineseNLPProcessor
        
        # åˆ›å»ºNLPå¤„ç†å™¨
        nlp_processor = ChineseNLPProcessor()
        
        # æµ‹è¯•æ–‡æœ¬
        test_text = "å¼ ä¸‰åœ¨åŒ—äº¬å¤§å­¦å·¥ä½œï¼Œä»–æ˜¯è®¡ç®—æœºç§‘å­¦ç³»çš„æ•™æˆã€‚æå››æ˜¯ä»–çš„åŒäº‹ï¼Œåœ¨åŒä¸€ä¸ªç³»å·¥ä½œã€‚"
        
        # æŠ½å–å®ä½“
        entities = nlp_processor.extract_entities(test_text)
        print(f"âœ“ å®ä½“æŠ½å–æˆåŠŸï¼Œå‘ç° {len(entities)} ä¸ªå®ä½“:")
        for entity in entities[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"  - {entity.text} ({entity.entity_type.value}, ç½®ä¿¡åº¦: {entity.confidence:.2f})")
        
        # æŠ½å–å…³ç³»
        relations = nlp_processor.extract_relations(test_text, entities)
        print(f"âœ“ å…³ç³»æŠ½å–æˆåŠŸï¼Œå‘ç° {len(relations)} ä¸ªå…³ç³»:")
        for relation in relations[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"  - {relation.source_entity.text} -> {relation.target_entity.text} ({relation.relation_type.value})")
        
        return True
        
    except Exception as e:
        print(f"âœ— NLPå¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_entity_extraction_service():
    """æµ‹è¯•å®ä½“æŠ½å–æœåŠ¡"""
    print("\n=== æµ‹è¯•å®ä½“æŠ½å–æœåŠ¡ ===")
    
    try:
        from src.knowledge_management.application.entity_extraction_service import EntityExtractionService
        
        # åˆ›å»ºæœåŠ¡
        service = EntityExtractionService()
        
        # æµ‹è¯•æ–‡æœ¬
        test_text = """å¾®è½¯å…¬å¸æ˜¯ä¸€å®¶ç¾å›½è·¨å›½ç§‘æŠ€å…¬å¸ï¼Œæ€»éƒ¨ä½äºåç››é¡¿å·é›·å¾·è’™å¾·ã€‚
æ¯”å°”Â·ç›–èŒ¨å’Œä¿ç½—Â·è‰¾ä¼¦äº1975å¹´åˆ›ç«‹äº†å¾®è½¯å…¬å¸ã€‚
è¨è’‚äºšÂ·çº³å¾·æ‹‰æ˜¯ç°ä»»é¦–å¸­æ‰§è¡Œå®˜ï¼Œäº2014å¹´æ¥ä»»è¿™ä¸€èŒä½ã€‚
å¾®è½¯å…¬å¸çš„ä¸»è¦äº§å“åŒ…æ‹¬Windowsæ“ä½œç³»ç»Ÿã€OfficeåŠå…¬è½¯ä»¶å¥—ä»¶å’ŒAzureäº‘æœåŠ¡ã€‚"""
        
        # ä»æ–‡æœ¬æŠ½å–
        result = service.extract_from_text(test_text)
        
        print(f"âœ“ å®ä½“æŠ½å–æœåŠ¡æµ‹è¯•æˆåŠŸ")
        print(f"  - å¤„ç†æ—¶é—´: {result.processing_time:.2f} ç§’")
        print(f"  - æŠ½å–å®ä½“æ•°: {len(result.entities)}")
        print(f"  - æŠ½å–å…³ç³»æ•°: {len(result.relations)}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = result.get_statistics()
        print(f"  - å®ä½“ç±»å‹åˆ†å¸ƒ: {stats['entity_type_counts']}")
        print(f"  - å…³ç³»ç±»å‹åˆ†å¸ƒ: {stats['relation_type_counts']}")
        
        return True
        
    except Exception as e:
        print(f"âœ— å®ä½“æŠ½å–æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_ml_enhancement_service():
    """æµ‹è¯•æœºå™¨å­¦ä¹ å¢å¼ºæœåŠ¡"""
    print("\n=== æµ‹è¯•æœºå™¨å­¦ä¹ å¢å¼ºæœåŠ¡ ===")
    
    try:
        from src.knowledge_management.application.ml_enhancement_service import MLEnhancementService
        from src.knowledge_management.domain.model.extraction import (
            ExtractedEntity, ExtractedRelation, EntityType, RelationType
        )
        
        # åˆ›å»ºæœåŠ¡
        service = MLEnhancementService()
        
        # åˆ›å»ºæµ‹è¯•å®ä½“
        entities = [
            ExtractedEntity(
                entity_id="1",
                text="å¼ ä¸‰",
                entity_type=EntityType.PERSON,
                confidence=0.9,
                start_pos=0,
                end_pos=2
            ),
            ExtractedEntity(
                entity_id="2",
                text="å¼ ä¸‰",  # é‡å¤å®ä½“
                entity_type=EntityType.PERSON,
                confidence=0.8,
                start_pos=10,
                end_pos=12
            ),
            ExtractedEntity(
                entity_id="3",
                text="åŒ—äº¬å¤§å­¦",
                entity_type=EntityType.ORGANIZATION,
                confidence=0.95,
                start_pos=5,
                end_pos=9
            )
        ]
        
        # åˆ›å»ºæµ‹è¯•å…³ç³»
        relations = [
            ExtractedRelation(
                relation_id="r1",
                source_entity=entities[0],
                target_entity=entities[2],
                relation_type=RelationType.WORK_FOR,
                confidence=0.8
            )
        ]
        
        # æµ‹è¯•å®ä½“å¯¹é½
        alignment_result = service.align_entities(entities)
        print(f"âœ“ å®ä½“å¯¹é½å®Œæˆï¼Œå‘ç° {len(alignment_result.aligned_groups)} ä¸ªå¯¹é½ç»„")
        
        # æµ‹è¯•è¯­ä¹‰æ¶ˆè§£
        disambiguation_result = service.disambiguate_entities(entities)
        print(f"âœ“ è¯­ä¹‰æ¶ˆè§£å®Œæˆï¼Œæ¶ˆè§£ {len(disambiguation_result.disambiguated_entities)} ä¸ªå®ä½“")
        
        # æµ‹è¯•å…³ç³»æ¨ç†
        inferred_relations = service.infer_relations(entities, relations)
        print(f"âœ“ å…³ç³»æ¨ç†å®Œæˆï¼Œæ¨ç†å‡º {len(inferred_relations)} ä¸ªæ–°å…³ç³»")
        
        # æµ‹è¯•å¼‚å¸¸æ£€æµ‹
        anomalies = service.detect_anomalies(entities, relations)
        total_anomalies = sum(len(anomaly_list) for anomaly_list in anomalies.values())
        print(f"âœ“ å¼‚å¸¸æ£€æµ‹å®Œæˆï¼Œå‘ç° {total_anomalies} ä¸ªå¼‚å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âœ— æœºå™¨å­¦ä¹ å¢å¼ºæœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_email_knowledge_service():
    """æµ‹è¯•é‚®ä»¶çŸ¥è¯†æŠ½å–æœåŠ¡"""
    print("\n=== æµ‹è¯•é‚®ä»¶çŸ¥è¯†æŠ½å–æœåŠ¡ ===")
    
    try:
        from src.email_ingestion.application.email_knowledge_service import EmailKnowledgeService
        from src.email_ingestion.domain.model.email import Email
        
        # åˆ›å»ºæœåŠ¡
        service = EmailKnowledgeService()
        
        # åˆ›å»ºæµ‹è¯•é‚®ä»¶
        test_email = Email(
            subject="é¡¹ç›®è¿›åº¦ä¼šè®®é€šçŸ¥",
            sender="project.manager@company.com",
            content="""å„ä½å›¢é˜Ÿæˆå‘˜ï¼Œ
            
æˆ‘ä»¬å°†äºæ˜å¤©ä¸‹åˆ2ç‚¹åœ¨ä¼šè®®å®¤Aå¬å¼€é¡¹ç›®è¿›åº¦ä¼šè®®ã€‚
è¯·å„ä½é¡¹ç›®æˆå‘˜å‡†æ—¶å‚åŠ ï¼Œä¼šè®®å°†è®¨è®ºä»¥ä¸‹å†…å®¹ï¼š
1. å½“å‰é¡¹ç›®è¿›å±•æƒ…å†µ
2. é‡åˆ°çš„æŠ€æœ¯éš¾é¢˜
3. ä¸‹é˜¶æ®µå·¥ä½œå®‰æ’

è¯·å¤§å®¶æå‰å‡†å¤‡ç›¸å…³ææ–™ã€‚

é¡¹ç›®ç»ç†
å¼ ä¸‰""",
            attachments=[{"filename": "é¡¹ç›®è¿›åº¦æŠ¥å‘Š.docx"}]
        )
        
        # ä»é‚®ä»¶æŠ½å–çŸ¥è¯†
        result = service.extract_knowledge_from_email(test_email)
        
        print(f"âœ“ é‚®ä»¶çŸ¥è¯†æŠ½å–æˆåŠŸ")
        print(f"  - å¤„ç†æ—¶é—´: {result.processing_time:.2f} ç§’")
        print(f"  - æŠ½å–å®ä½“æ•°: {len(result.entities)}")
        print(f"  - æŠ½å–å…³ç³»æ•°: {len(result.relations)}")
        print(f"  - é‚®ä»¶å‘ä»¶äºº: {result.metadata['email_sender']}")
        print(f"  - é‚®ä»¶ä¸»é¢˜: {result.metadata['email_subject']}")
        
        # æ˜¾ç¤ºæŠ½å–çš„å®ä½“
        print("  - ä¸»è¦å®ä½“:")
        for entity in result.entities[:5]:
            print(f"    * {entity.text} ({entity.entity_type.value})")
        
        return True
        
    except Exception as e:
        print(f"âœ— é‚®ä»¶çŸ¥è¯†æŠ½å–æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_integrated_knowledge_service():
    """æµ‹è¯•é›†æˆçŸ¥è¯†æœåŠ¡"""
    print("\n=== æµ‹è¯•é›†æˆçŸ¥è¯†æœåŠ¡ ===")
    
    try:
        from src.knowledge_management.application.integrated_knowledge_service import IntegratedKnowledgeService
        from src.email_ingestion.domain.model.email import Email
        
        # åˆ›å»ºæœåŠ¡
        service = IntegratedKnowledgeService()
        
        # è·å–æœåŠ¡çŠ¶æ€
        status = service.get_service_status()
        print(f"âœ“ æœåŠ¡çŠ¶æ€æ£€æŸ¥å®Œæˆ")
        print(f"  - æœåŠ¡åç§°: {status['service_name']}")
        print(f"  - æœåŠ¡çŠ¶æ€: {status['status']}")
        print(f"  - æ”¯æŒåŠŸèƒ½: {', '.join(status['supported_features'])}")
        
        # æµ‹è¯•é‚®ä»¶å¤„ç†
        test_emails = [
            Email(
                subject="æŠ€æœ¯è®¨è®º",
                sender="developer1@company.com",
                content="å…³äºæ–°æ¶æ„çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œæˆ‘è®¤ä¸ºåº”è¯¥é‡‡ç”¨å¾®æœåŠ¡æ¶æ„ã€‚",
                attachments=[]
            ),
            Email(
                subject="é¡¹ç›®æ›´æ–°",
                sender="manager@company.com",
                content="é¡¹ç›®è¿›å±•é¡ºåˆ©ï¼Œé¢„è®¡ä¸‹å‘¨å®Œæˆç¬¬ä¸€é˜¶æ®µå¼€å‘ã€‚",
                attachments=[]
            )
        ]
        
        # å¤„ç†é‚®ä»¶åˆ°çŸ¥è¯†å›¾è°±
        result = service.process_emails_to_knowledge_graph(
            test_emails, enable_ml_enhancement=False
        )
        
        print(f"âœ“ é‚®ä»¶çŸ¥è¯†å›¾è°±å¤„ç†å®Œæˆ")
        print(f"  - å¤„ç†é‚®ä»¶æ•°: {result['processing_summary']['total_emails']}")
        print(f"  - æ€»å®ä½“æ•°: {result['processing_summary']['total_entities']}")
        print(f"  - æ€»å…³ç³»æ•°: {result['processing_summary']['total_relations']}")
        print(f"  - çŸ¥è¯†å›¾è°±èŠ‚ç‚¹æ•°: {result['knowledge_graph']['nodes_count']}")
        print(f"  - çŸ¥è¯†å›¾è°±è¾¹æ•°: {result['knowledge_graph']['edges_count']}")
        
        return True
        
    except Exception as e:
        print(f"âœ— é›†æˆçŸ¥è¯†æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def create_demo_data():
    """åˆ›å»ºæ¼”ç¤ºæ•°æ®"""
    print("\n=== åˆ›å»ºæ¼”ç¤ºæ•°æ® ===")
    
    # åˆ›å»ºæ¼”ç¤ºæ–‡æ¡£
    demo_content = """äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•æŠ¥å‘Š

æ¦‚è¿°ï¼š
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚

ä¸»è¦å…¬å¸å’Œäººç‰©ï¼š
1. OpenAIå…¬å¸ç”±è¨å§†Â·å¥¥ç‰¹æ›¼é¢†å¯¼ï¼Œå¼€å‘äº†ChatGPTç­‰äº§å“
2. è°·æ­Œå…¬å¸çš„DeepMindå›¢é˜Ÿåœ¨æœºå™¨å­¦ä¹ é¢†åŸŸå–å¾—é‡å¤§çªç ´
3. å¾®è½¯å…¬å¸ä¸OpenAIå»ºç«‹äº†æˆ˜ç•¥åˆä½œå…³ç³»
4. ç‰¹æ–¯æ‹‰å…¬å¸çš„åŸƒéš†Â·é©¬æ–¯å…‹ä¹Ÿåœ¨AIé¢†åŸŸæœ‰é‡è¦æŠ•èµ„

æŠ€æœ¯å‘å±•ï¼š
æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚
å¤§è¯­è¨€æ¨¡å‹å¦‚GPT-4å±•ç°äº†å¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚

åº”ç”¨é¢†åŸŸï¼š
- è‡ªåŠ¨é©¾é©¶ï¼šç‰¹æ–¯æ‹‰ã€ç™¾åº¦ç­‰å…¬å¸åœ¨æ­¤é¢†åŸŸæŠ•å…¥å·¨å¤§
- åŒ»ç–—è¯Šæ–­ï¼šAIè¾…åŠ©åŒ»ç”Ÿè¿›è¡Œç–¾ç—…è¯Šæ–­
- é‡‘èæœåŠ¡ï¼šæ™ºèƒ½æŠ•é¡¾ã€é£é™©æ§åˆ¶ç­‰
- æ•™è‚²ï¼šä¸ªæ€§åŒ–å­¦ä¹ ã€æ™ºèƒ½è¾…å¯¼

æœªæ¥å±•æœ›ï¼š
äººå·¥æ™ºèƒ½å°†ç»§ç»­å¿«é€Ÿå‘å±•ï¼Œé¢„è®¡åœ¨æœªæ¥5-10å¹´å†…å°†åœ¨æ›´å¤šé¢†åŸŸå®ç°çªç ´ã€‚
"""
    
    demo_file = "demo_ai_report.txt"
    with open(demo_file, 'w', encoding='utf-8') as f:
        f.write(demo_content)
    
    print(f"âœ“ åˆ›å»ºæ¼”ç¤ºæ–‡æ¡£: {demo_file}")
    return demo_file

def run_full_demo():
    """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
    print("\n=== è¿è¡Œå®Œæ•´çŸ¥è¯†å›¾è°±ç³»ç»Ÿæ¼”ç¤º ===")
    
    try:
        from src.knowledge_management.application.integrated_knowledge_service import IntegratedKnowledgeService
        
        # åˆ›å»ºæ¼”ç¤ºæ•°æ®
        demo_file = create_demo_data()
        
        # åˆ›å»ºé›†æˆæœåŠ¡
        service = IntegratedKnowledgeService()
        
        # å¤„ç†æ–‡æ¡£åˆ°çŸ¥è¯†å›¾è°±
        print("\næ­£åœ¨å¤„ç†æ–‡æ¡£...")
        result = service.process_documents_to_knowledge_graph(
            [demo_file],
            enable_ml_enhancement=False,  # æš‚æ—¶ç¦ç”¨MLå¢å¼ºä»¥é¿å…ä¾èµ–é—®é¢˜
            custom_entity_types={
                'COMPANY': ['OpenAI', 'è°·æ­Œ', 'å¾®è½¯', 'ç‰¹æ–¯æ‹‰', 'ç™¾åº¦'],
                'TECHNOLOGY': ['äººå·¥æ™ºèƒ½', 'æ·±åº¦å­¦ä¹ ', 'æœºå™¨å­¦ä¹ ', 'ChatGPT', 'GPT-4']
            }
        )
        
        print(f"\nâœ“ æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        print(f"  - çŸ¥è¯†å›¾è°±èŠ‚ç‚¹æ•°: {result['knowledge_graph']['nodes_count']}")
        print(f"  - çŸ¥è¯†å›¾è°±è¾¹æ•°: {result['knowledge_graph']['edges_count']}")
        
        # å¯¼å‡ºç»“æœ
        if result['knowledge_graph'].get('graph'):
            output_file = "demo_knowledge_graph.json"
            try:
                service.export_knowledge_graph(result['knowledge_graph']['graph'], output_file)
                print(f"  - çŸ¥è¯†å›¾è°±å·²å¯¼å‡ºåˆ°: {output_file}")
            except Exception as export_error:
                print(f"  - çŸ¥è¯†å›¾è°±å¯¼å‡ºè·³è¿‡: {export_error}")
        
        # æ˜¾ç¤ºæœ¬ä½“ä¿¡æ¯
        if 'ontology' in result:
            ontology = result['ontology']
            if hasattr(ontology, 'get_statistics'):
                ontology_stats = ontology.get_statistics()
                print(f"  - æœ¬ä½“ç±»æ•°é‡: {ontology_stats.get('classes_count', 0)}")
                print(f"  - æœ¬ä½“å…³ç³»æ•°é‡: {ontology_stats.get('relations_count', 0)}")
        
        # æ¸…ç†æ¼”ç¤ºæ–‡ä»¶
        os.remove(demo_file)
        
        return True
        
    except Exception as e:
        print(f"âœ— å®Œæ•´æ¼”ç¤ºå¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("çŸ¥è¯†å›¾è°±å¯è§†åŒ–ä¸ç®¡ç†å¹³å° - ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # æµ‹è¯•ç»“æœç»Ÿè®¡
    test_results = []
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_results.append(("æ–‡æ¡£è§£æ", test_document_parser()))
    test_results.append(("NLPå¤„ç†", test_nlp_processor()))
    test_results.append(("å®ä½“æŠ½å–æœåŠ¡", test_entity_extraction_service()))
    test_results.append(("MLå¢å¼ºæœåŠ¡", test_ml_enhancement_service()))
    test_results.append(("é‚®ä»¶çŸ¥è¯†æŠ½å–", test_email_knowledge_service()))
    test_results.append(("é›†æˆçŸ¥è¯†æœåŠ¡", test_integrated_knowledge_service()))
    
    # è¿è¡Œå®Œæ•´æ¼”ç¤º
    test_results.append(("å®Œæ•´æ¼”ç¤º", run_full_demo()))
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ‘˜è¦:")
    print("=" * 50)
    
    passed = 0
    failed = 0
    
    for test_name, result in test_results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{test_name:<20} {status}")
        if result:
            passed += 1
        else:
            failed += 1
    
    print(f"\næ€»è®¡: {passed} ä¸ªæµ‹è¯•é€šè¿‡, {failed} ä¸ªæµ‹è¯•å¤±è´¥")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼ç³»ç»ŸåŠŸèƒ½æ­£å¸¸ã€‚")
    else:
        print(f"\nâš ï¸  æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
    
    print("\næ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main()