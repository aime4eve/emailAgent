#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³»ç»Ÿæµ‹è¯•è„šæœ¬
æµ‹è¯•é‚®ä»¶æ™ºèƒ½ä»£ç†ç³»ç»Ÿçš„æ‰€æœ‰åŠŸèƒ½æ¨¡å—
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_knowledge_graph():
    """æµ‹è¯•çŸ¥è¯†å›¾è°±æ ¸å¿ƒåŠŸèƒ½"""
    print("\n=== æµ‹è¯•çŸ¥è¯†å›¾è°±åŠŸèƒ½ ===")
    try:
        from src.knowledge_graph.core.graph import KnowledgeGraph
        from src.knowledge_graph.core.node import Node
        from src.knowledge_graph.core.edge import Edge
        
        # åˆ›å»ºçŸ¥è¯†å›¾è°±
        kg = KnowledgeGraph()
        
        # æ·»åŠ èŠ‚ç‚¹
        node1 = Node("person_1", "å¼ ä¸‰", "PERSON", {"age": 30, "job": "æ•™æˆ"})
        node2 = Node("org_1", "åŒ—äº¬å¤§å­¦", "ORGANIZATION", {"type": "å¤§å­¦", "location": "åŒ—äº¬"})
        
        kg.add_node(node1)
        kg.add_node(node2)
        
        # æ·»åŠ å…³ç³»
        edge = Edge("rel_1", node1.id, node2.id, "WORKS_AT", {"since": "2020"})
        kg.add_edge(edge)
        
        # éªŒè¯
        assert len(kg.get_all_nodes()) == 2
        assert len(kg.get_all_edges()) == 1
        
        print("âœ“ çŸ¥è¯†å›¾è°±åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— çŸ¥è¯†å›¾è°±æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_nlp_processing():
    """æµ‹è¯•NLPå¤„ç†åŠŸèƒ½"""
    print("\n=== æµ‹è¯•NLPå¤„ç†åŠŸèƒ½ ===")
    try:
        from src.nlp_processing.text_preprocessor import TextPreprocessor
        from src.nlp_processing.entity_extractor import EntityExtractor
        from src.nlp_processing.relation_extractor import RelationExtractor
        
        # æµ‹è¯•æ–‡æœ¬é¢„å¤„ç†
        preprocessor = TextPreprocessor()
        text = "å¼ ä¸‰æ˜¯åŒ—äº¬å¤§å­¦çš„æ•™æˆï¼Œä»–åœ¨è®¡ç®—æœºç§‘å­¦ç³»å·¥ä½œã€‚"
        
        tokens = preprocessor.tokenize(text)
        sentences = preprocessor.extract_sentences(text)
        
        print(f"âœ“ æ–‡æœ¬é¢„å¤„ç†: {len(tokens)} ä¸ªè¯ï¼Œ{len(sentences)} ä¸ªå¥å­")
        
        # æµ‹è¯•å®ä½“æŠ½å–
        extractor = EntityExtractor()
        entities = extractor.extract_entities(text)
        
        print(f"âœ“ å®ä½“æŠ½å–: å‘ç° {len(entities)} ä¸ªå®ä½“")
        
        # æµ‹è¯•å…³ç³»æŠ½å–
        relation_extractor = RelationExtractor()
        relations = relation_extractor.extract_relations(text, entities)
        
        print(f"âœ“ å…³ç³»æŠ½å–: å‘ç° {len(relations)} ä¸ªå…³ç³»")
        
        return True
        
    except Exception as e:
        print(f"âœ— NLPå¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_ml_enhancement():
    """æµ‹è¯•æœºå™¨å­¦ä¹ å¢å¼ºåŠŸèƒ½"""
    print("\n=== æµ‹è¯•æœºå™¨å­¦ä¹ å¢å¼ºåŠŸèƒ½ ===")
    try:
        from src.ml_enhancement.similarity_calculator import SimilarityCalculator
        from src.ml_enhancement.entity_alignment import EntityAlignment
        
        # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
        calc = SimilarityCalculator()
        
        similarity = calc.jaccard_similarity_text("åŒ—äº¬å¤§å­¦", "æ¸…åå¤§å­¦")
        print(f"âœ“ ç›¸ä¼¼åº¦è®¡ç®—: Jaccardç›¸ä¼¼åº¦ = {similarity:.3f}")
        
        # æµ‹è¯•å®ä½“å¯¹é½
        alignment = EntityAlignment()
        print("âœ“ å®ä½“å¯¹é½æ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âœ— æœºå™¨å­¦ä¹ å¢å¼ºæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_knowledge_extraction_service():
    """æµ‹è¯•çŸ¥è¯†æŠ½å–æœåŠ¡"""
    print("\n=== æµ‹è¯•çŸ¥è¯†æŠ½å–æœåŠ¡ ===")
    try:
        from src.knowledge_management.application.knowledge_extraction_service import KnowledgeExtractionService
        
        service = KnowledgeExtractionService()
        
        # æµ‹è¯•æ–‡æœ¬çŸ¥è¯†æŠ½å–
        text = "å¼ ä¸‰æ˜¯åŒ—äº¬å¤§å­¦çš„æ•™æˆï¼Œä¸“é—¨ç ”ç©¶äººå·¥æ™ºèƒ½ã€‚æå››æ˜¯æ¸…åå¤§å­¦çš„å­¦ç”Ÿã€‚"
        result = service.extract_from_text(text)
        
        print(f"âœ“ çŸ¥è¯†æŠ½å–æœåŠ¡: å¤„ç†æ–‡æœ¬æˆåŠŸ")
        print(f"  - å®ä½“æ•°é‡: {len(result.get('entities', []))}")
        print(f"  - å…³ç³»æ•°é‡: {len(result.get('relations', []))}")
        
        return True
        
    except Exception as e:
        print(f"âœ— çŸ¥è¯†æŠ½å–æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_web_interface():
    """æµ‹è¯•Webæ¥å£"""
    print("\n=== æµ‹è¯•Webæ¥å£ ===")
    try:
        # æ£€æŸ¥Flaskåº”ç”¨æ˜¯å¦å¯ä»¥å¯åŠ¨
        from src.web.app import create_app
        
        app = create_app()
        
        with app.test_client() as client:
            # æµ‹è¯•ä¸»é¡µ
            response = client.get('/')
            assert response.status_code == 200
            
            print("âœ“ Webæ¥å£æµ‹è¯•é€šè¿‡")
            return True
            
    except Exception as e:
        print(f"âœ— Webæ¥å£æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_document_processing():
    """æµ‹è¯•æ–‡æ¡£å¤„ç†åŠŸèƒ½"""
    print("\n=== æµ‹è¯•æ–‡æ¡£å¤„ç†åŠŸèƒ½ ===")
    try:
        from src.nlp_processing.document_parser import DocumentParser
        
        parser = DocumentParser()
        
        # åˆ›å»ºæµ‹è¯•æ–‡æœ¬æ–‡ä»¶
        test_file = "test_document.txt"
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚å¼ ä¸‰æ˜¯åŒ—äº¬å¤§å­¦çš„æ•™æˆã€‚")
        
        try:
            # æµ‹è¯•æ–‡æ¡£è§£æ
            result = parser.parse_document(test_file)
            
            print(f"âœ“ æ–‡æ¡£è§£æ: æå–æ–‡æœ¬é•¿åº¦ {len(result['content'])} å­—ç¬¦")
            
            return True
            
        finally:
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            if os.path.exists(test_file):
                os.remove(test_file)
                
    except Exception as e:
        print(f"âœ— æ–‡æ¡£å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def generate_test_report(results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*50)
    print("ç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š")
    print("="*50)
    
    total_tests = len(results)
    passed_tests = sum(1 for result in results.values() if result)
    
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
    print(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")
    
    print("\nè¯¦ç»†ç»“æœ:")
    for test_name, result in results.items():
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report = {
        "timestamp": str(Path(__file__).stat().st_mtime),
        "total_tests": total_tests,
        "passed_tests": passed_tests,
        "pass_rate": passed_tests/total_tests*100,
        "results": results
    }
    
    with open("test_report.json", 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\næµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: test_report.json")
    
    return passed_tests == total_tests

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹ç³»ç»Ÿæµ‹è¯•...")
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_results = {
        "çŸ¥è¯†å›¾è°±åŠŸèƒ½": test_knowledge_graph(),
        "NLPå¤„ç†åŠŸèƒ½": test_nlp_processing(),
        "æœºå™¨å­¦ä¹ å¢å¼º": test_ml_enhancement(),
        "çŸ¥è¯†æŠ½å–æœåŠ¡": test_knowledge_extraction_service(),
        "æ–‡æ¡£å¤„ç†åŠŸèƒ½": test_document_processing(),
        "Webæ¥å£": test_web_interface()
    }
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    all_passed = generate_test_report(test_results)
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»ŸåŠŸèƒ½æ­£å¸¸ã€‚")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ¨¡å—ã€‚")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)